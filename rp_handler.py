import runpod
import os
import requests
import time
import json
import base64
from PIL import Image
import io

def generate_pepe_with_lora(prompt):
    """Generate Pepe image using LoRA in ComfyUI workflow"""
    try:
        print(f"üê∏ Generating REAL Pepe with LoRA: {prompt}")
        
        # UPDATED WORKFLOW WITH PEPE LORA
        workflow = {
            "1": {
                "inputs": {"ckpt_name": "sd15.safetensors"},
                "class_type": "CheckpointLoaderSimple"
            },
            "2": {
                "inputs": {
                    "lora_name": "pepe.safetensors",
                    "strength_model": 1.0,
                    "strength_clip": 1.0,
                    "model": ["1", 0],
                    "clip": ["1", 1]
                },
                "class_type": "LoraLoader"
            },
            "3": {
                "inputs": {
                    "text": f"pepe the frog, {prompt}, meme style, simple cartoon, green frog character",
                    "clip": ["2", 1]  # Use LoRA-modified CLIP
                },
                "class_type": "CLIPTextEncode"
            },
            "4": {
                "inputs": {
                    "text": "blurry, low quality, distorted, realistic, photorealistic, human, anime, complex background",
                    "clip": ["2", 1]  # Use LoRA-modified CLIP
                },
                "class_type": "CLIPTextEncode"
            },
            "5": {
                "inputs": {
                    "width": 512,
                    "height": 512,
                    "batch_size": 1
                },
                "class_type": "EmptyLatentImage"
            },
            "6": {
                "inputs": {
                    "seed": int(time.time()) % 1000000,
                    "steps": 30,  # More steps for LoRA
                    "cfg": 8.0,   # Higher CFG for LoRA
                    "sampler_name": "euler_ancestral",
                    "scheduler": "normal",
                    "denoise": 1.0,
                    "model": ["2", 0],  # Use LoRA-modified model
                    "positive": ["3", 0],
                    "negative": ["4", 0],
                    "latent_image": ["5", 0]
                },
                "class_type": "KSampler"
            },
            "7": {
                "inputs": {
                    "samples": ["6", 0],
                    "vae": ["1", 2]
                },
                "class_type": "VAEDecode"
            },
            "8": {
                "inputs": {
                    "filename_prefix": f"real_pepe_{int(time.time())}",
                    "images": ["7", 0]
                },
                "class_type": "SaveImage"
            }
        }
        
        # Submit generation request
        response = requests.post(
            "http://localhost:8188/prompt",
            json={"prompt": workflow},
            timeout=30
        )
        
        if response.status_code != 200:
            return {"error": f"Generation request failed: {response.status_code}"}
        
        prompt_data = response.json()
        prompt_id = prompt_data.get("prompt_id")
        
        if not prompt_id:
            return {"error": "No prompt ID returned"}
        
        print(f"‚úÖ Pepe LoRA generation queued: {prompt_id}")
        
        # Wait for completion with longer timeout for LoRA
        max_wait = 180  # 3 minutes for LoRA generation
        for i in range(max_wait):
            try:
                # Check queue status
                queue_response = requests.get("http://localhost:8188/queue", timeout=5)
                if queue_response.status_code == 200:
                    queue_data = queue_response.json()
                    
                    # Check if our prompt is still in queue
                    running = queue_data.get("queue_running", [])
                    pending = queue_data.get("queue_pending", [])
                    
                    still_processing = any(
                        item[1].get("prompt_id") == prompt_id 
                        for item in running + pending
                        if len(item) > 1 and isinstance(item[1], dict)
                    )
                    
                    if not still_processing:
                        print("üéâ Pepe LoRA generation completed!")
                        break
                
                if i % 15 == 0:
                    print(f"‚è≥ Pepe LoRA generating... ({i}/{max_wait}s)")
                
                time.sleep(1)
                
            except Exception as e:
                print(f"‚ö†Ô∏è Queue check error: {e}")
                time.sleep(1)
        
        # Find generated images
        output_dir = "/workspace/comfyui/output"
        if not os.path.exists(output_dir):
            return {"error": "Output directory not found"}
        
        # Look for recent images with our prefix
        image_files = []
        for root, dirs, files in os.walk(output_dir):
            for file in files:
                if file.lower().endswith(('.png', '.jpg', '.jpeg')) and "real_pepe_" in file:
                    file_path = os.path.join(root, file)
                    # Check if file was created recently
                    if time.time() - os.path.getctime(file_path) < 300:
                        image_files.append(file_path)
        
        if not image_files:
            # Fallback: look for any recent images
            for root, dirs, files in os.walk(output_dir):
                for file in files:
                    if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                        file_path = os.path.join(root, file)
                        if time.time() - os.path.getctime(file_path) < 300:
                            image_files.append(file_path)
        
        if not image_files:
            return {"error": "No Pepe images found", "output_dir": output_dir}
        
        # Use the most recent image
        latest_image = max(image_files, key=os.path.getctime)
        
        # Convert to base64
        try:
            with open(latest_image, 'rb') as img_file:
                img_data = img_file.read()
                img_base64 = base64.b64encode(img_data).decode('utf-8')
            
            # Get image info
            with Image.open(latest_image) as img:
                width, height = img.size
                format = img.format
            
        except Exception as e:
            return {"error": f"Failed to process Pepe image: {e}"}
        
        return {
            "success": True,
            "image_path": latest_image,
            "image_base64": img_base64,
            "image_info": {
                "width": width,
                "height": height,
                "format": format,
                "size_bytes": len(img_data)
            },
            "prompt_id": prompt_id,
            "lora_used": "pepe.safetensors",
            "generation_method": "Real Pepe LoRA"
        }
        
    except Exception as e:
        return {"error": f"Pepe LoRA generation failed: {str(e)}"}

def handler(event):
    """Enhanced Pepe generator with proper LoRA usage"""
    print("üê∏ REAL PEPE GENERATOR v11.0 - WITH LORA! üéØ")
    
    try:
        input_data = event.get('input', {})
        prompt = input_data.get('prompt', 'pepe the frog')
        action = input_data.get('action', 'generate')
        
        if action == 'status':
            # Return status
            return {
                "message": "üê∏ Real Pepe Generator Status",
                "pepe_lora": "‚úÖ Available (171MB)",
                "sd15_model": "‚úÖ Available (4.2GB)", 
                "server": "‚úÖ Running",
                "gpu": "‚úÖ RTX 4090 ready",
                "ready_for_pepe": True
            }
        
        # Generate REAL Pepe with LoRA
        print(f"üìù Generating REAL Pepe: {prompt}")
        
        generation_result = generate_pepe_with_lora(prompt)
        
        if generation_result.get("success"):
            return {
                "message": f"üê∏ REAL PEPE GENERATED!",
                "prompt": prompt,
                "generation": generation_result,
                "lora_info": {
                    "lora_file": "pepe.safetensors",
                    "strength": "1.0 (full strength)",
                    "method": "ComfyUI LoRA integration"
                },
                "viewing_options": {
                    "base64_image": f"data:image/png;base64,{generation_result['image_base64'][:100]}...",
                    "image_path": generation_result["image_path"],
                    "instructions": "This should be a REAL Pepe with proper LoRA training!"
                },
                "ready_for_generation": True
            }
        else:
            return {
                "error": "Real Pepe generation failed",
                "details": generation_result,
                "suggestion": "Check if LoRA file exists and ComfyUI can load it"
            }
        
    except Exception as e:
        return {
            "error": str(e),
            "debug": "Handler exception in Real Pepe mode"
        }

if __name__ == '__main__':
    print("üöÄ Starting REAL Pepe Generator with LoRA...")
    runpod.serverless.start({'handler': handler})
